{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97977429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve, auc, precision_recall_curve, log_loss, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "998e5e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amimo\\anaconda3\\envs\\ML-GPU\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\amimo\\anaconda3\\envs\\ML-GPU\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector length: 907\n",
      "953\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\n",
    "from transformers import ViTFeatureExtractor, ViTModel, ViTConfig\n",
    "from skimage.feature import local_binary pattern\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define additional transformer block\n",
    "class ExtraTransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ExtraTransformerBlock, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)  # Add one extra block\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        return self.transformer_encoder(hidden_states)\n",
    "\n",
    "# Modified ViT model with an additional Transformer block\n",
    "class ModifiedViTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedViTModel, self).__init__()\n",
    "        # Load the pretrained ViT model\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        config = ViTConfig.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        # Add extra transformer block\n",
    "        self.extra_block = ExtraTransformerBlock(config)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Extract the original ViT features\n",
    "        outputs = self.vit(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # Pass through the additional transformer block\n",
    "        extra_features = self.extra_block(last_hidden_state)\n",
    "        return extra_features\n",
    "\n",
    "# Initialize the ViT feature extractor and the modified ViT model\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ModifiedViTModel()\n",
    "\n",
    "# Function to extract ViT features\n",
    "def extract_vit_features(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    inputs = feature_extractor(images=img, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "    return outputs.flatten().detach().numpy()\n",
    "\n",
    "# Other functions remain unchanged\n",
    "def extract_sift_features(img):\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "    if descriptors is None:\n",
    "        descriptors = np.zeros((1, 128))  # Assuming 128-dimensional descriptors\n",
    "    return descriptors\n",
    "\n",
    "def extract_lbp_features(img):\n",
    "    lbp = local_binary_pattern(img, P=8, R=1, method='uniform')\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, 11), range=(0, 10))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)\n",
    "    return hist\n",
    "\n",
    "def concatenate_features(sift_features, lbp_features, vit_features):\n",
    "    sift_features = sift_features.flatten() if sift_features is not None else np.zeros(128)\n",
    "    lbp_features = lbp_features.flatten() if lbp_features is not None else np.zeros(11)\n",
    "    vit_features = vit_features.flatten() if vit_features is not None else np.zeros(768)\n",
    "\n",
    "    combined_features = np.concatenate((sift_features, lbp_features, vit_features))\n",
    "\n",
    "    fixed_length = 907  # Ensure the combined feature vector has a consistent size\n",
    "    if len(combined_features) < fixed_length:\n",
    "        combined_features = np.concatenate((combined_features, np.zeros(fixed_length - len(combined_features))))\n",
    "    elif len(combined_features) > fixed_length:\n",
    "        combined_features = combined_features[:fixed_length]\n",
    "\n",
    "    return combined_features\n",
    "\n",
    "# Process images and extract features\n",
    "source_dir = './Cropped_Images/'\n",
    "features, labels, label_map, reverse_label_map = process_images_in_directory(source_dir)\n",
    "\n",
    "print(f\"Feature vector length: {len(features[0])}\")\n",
    "print(len(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d25c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label map: {'': 0, 'Person_001': 1, 'Person_002': 2, 'Person_003': 3, 'Person_004': 4, 'Person_005': 5, 'Person_006': 6, 'Person_007': 7, 'Person_008': 8, 'Person_009': 9, 'Person_010': 10, 'Person_011': 11, 'Person_012': 12, 'Person_013': 13, 'Person_014': 14, 'Person_015': 15, 'Person_016': 16, 'Person_017': 17, 'Person_018': 18, 'Person_019': 19, 'Person_020': 20, 'Person_021': 21, 'Person_022': 22, 'Person_023': 23, 'Person_024': 24, 'Person_025': 25, 'Person_026': 26, 'Person_027': 27, 'Person_028': 28, 'Person_029': 29, 'Person_030': 30, 'Person_031': 31, 'Person_032': 32, 'Person_033': 33, 'Person_034': 34, 'Person_035': 35, 'Person_036': 36, 'Person_037': 37, 'Person_038': 38, 'Person_039': 39, 'Person_040': 40, 'Person_041': 41, 'Person_042': 42, 'Person_043': 43, 'Person_044': 44, 'Person_045': 45, 'Person_046': 46, 'Person_047': 47, 'Person_048': 48, 'Person_049': 49, 'Person_050': 50, 'Person_051': 51, 'Person_052': 52, 'Person_053': 53, 'Person_054': 54, 'Person_055': 55, 'Person_056': 56, 'Person_057': 57, 'Person_058': 58, 'Person_059': 59, 'Person_060': 60, 'Person_061': 61, 'Person_062': 62, 'Person_063': 63, 'Person_064': 64, 'Person_065': 65, 'Person_066': 66, 'Person_067': 67, 'Person_068': 68, 'Person_069': 69, 'Person_070': 70, 'Person_071': 71, 'Person_072': 72, 'Person_073': 73, 'Person_074': 74, 'Person_075': 75, 'Person_076': 76, 'Person_077': 77, 'Person_078': 78, 'Person_079': 79, 'Person_080': 80, 'Person_081': 81, 'Person_082': 82}\n"
     ]
    }
   ],
   "source": [
    "print(\"Label map:\", label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0b7734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6238ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example padding to a fixed length\n",
    "fixed_length = 1000  # Define a fixed length\n",
    "features_padded = pad_sequences(features, maxlen=fixed_length, padding='post', dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0e866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84a31a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(953, 1000)\n",
      "(953,)\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to NumPy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Check the shapes again\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8a7414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform Kernel PCA\n",
    "kpca = KernelPCA(n_components=300, kernel='rbf')\n",
    "features = kpca.fit_transform(features)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c271d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, cross_val_predict\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import classification_report, roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "# from sklearn.decomposition import PCA, KernelPCA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "# from sklearn.metrics import RocCurveDisplay, DetCurveDisplay\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.decomposition import PCA, KernelPCA, SparsePCA, TruncatedSVD, FastICA, FactorAnalysis, NMF, IncrementalPCA\n",
    "# from sklearn.manifold import TSNE, MDS, Isomap, LocallyLinearEmbedding\n",
    "# from sklearn.random_projection import GaussianRandomProjection\n",
    "# import umap\n",
    "\n",
    "# # Create a directory to save plots and metrics if it doesn't exist\n",
    "# output_dir = \"model_outputs\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# features = scaler.fit_transform(features)\n",
    "\n",
    "# # Dimensionality Reduction and Feature Selection Techniques\n",
    "# def apply_pca(X, n_components=0.95):\n",
    "#     pca = PCA(n_components=n_components)\n",
    "#     return pca.fit_transform(X)\n",
    "\n",
    "# def apply_kernel_pca(X, n_components=300, kernel='rbf'):\n",
    "#     kpca = KernelPCA(n_components=n_components, kernel=kernel)\n",
    "#     return kpca.fit_transform(X)\n",
    "\n",
    "# def apply_lda(X, y, n_components=1):\n",
    "#     lda = LDA(n_components=n_components)\n",
    "#     return lda.fit_transform(X, y)\n",
    "\n",
    "# def apply_tsne(X, n_components=2):\n",
    "#     tsne = TSNE(n_components=n_components)\n",
    "#     return tsne.fit_transform(X)\n",
    "\n",
    "# def apply_ipca(X, n_components=5, batch_size=500):\n",
    "#     ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
    "#     return ipca.fit_transform(X)\n",
    "\n",
    "# def apply_grp(X, n_components=10):\n",
    "#     grp = GaussianRandomProjection(n_components=n_components)\n",
    "#     return grp.fit_transform(X)\n",
    "\n",
    "# def apply_umap(X, n_components=10):\n",
    "#     umap_model = umap.UMAP(n_components=n_components)\n",
    "#     return umap_model.fit_transform(X)\n",
    "\n",
    "\n",
    "# # umap_model = umap.UMAP(n_components=4)\n",
    "# # X_train = umap_model.fit_transform(X_train)\n",
    "\n",
    "\n",
    "\n",
    "# # Feature Selection and Dimensionality Reduction\n",
    "# X_pca = apply_pca(features)\n",
    "# X_kpca = apply_kernel_pca(features)\n",
    "# X_lda = apply_lda(features, labels)\n",
    "# X_tsne = apply_tsne(features)\n",
    "# X_ipca = apply_ipca(features)\n",
    "# X_grp = apply_grp(features)\n",
    "# X_umap = apply_umap(features)\n",
    "\n",
    "# # Classification Algorithms with L1 Regularization\n",
    "# models = {\n",
    "#     'SVM (L1)': CalibratedClassifierCV(LinearSVC(penalty='l1', dual=False, max_iter=10000)),\n",
    "#     'RandomForest': RandomForestClassifier(),\n",
    "#     'KNN': KNeighborsClassifier(),\n",
    "#     'LogisticRegression (L1)': LogisticRegression(penalty='l1', solver='liblinear'),\n",
    "#     'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "# }\n",
    "\n",
    "# # 5-Fold Cross Validation\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# def save_plot(fig, filename):\n",
    "#     file_path = os.path.join(output_dir, filename)\n",
    "#     fig.savefig(file_path)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# def evaluate_model(X, y, method_name):\n",
    "#     results = {}\n",
    "#     metrics_list = []\n",
    "\n",
    "#     for name, model in models.items():\n",
    "#         y_pred = cross_val_predict(model, X, y, cv=cv, method='predict')\n",
    "#         y_prob = cross_val_predict(model, X, y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "#         accuracy = accuracy_score(y, y_pred)\n",
    "#         precision = precision_score(y, y_pred)\n",
    "#         recall = recall_score(y, y_pred)\n",
    "#         f1 = f1_score(y, y_pred)\n",
    "\n",
    "#         # Print and collect metrics\n",
    "#         print(f'{method_name} - {name} - Accuracy: {accuracy}')\n",
    "#         print(f'{method_name} - {name} - Precision: {precision}')\n",
    "#         print(f'{method_name} - {name} - Recall: {recall}')\n",
    "#         print(f'{method_name} - {name} - F1 Score: {f1}')\n",
    "        \n",
    "#         metrics_list.append({\n",
    "#             'Method': method_name,\n",
    "#             'Model': name,\n",
    "#             'Accuracy': accuracy,\n",
    "#             'Precision': precision,\n",
    "#             'Recall': recall,\n",
    "#             'F1 Score': f1\n",
    "#         })\n",
    "\n",
    "#         # Storing Results\n",
    "#         results[name] = {\n",
    "#             'model': model,\n",
    "#             'y_pred': y_pred,\n",
    "#             'y_prob': y_prob\n",
    "#         }\n",
    "\n",
    "#         # ROC Curve\n",
    "#         fpr, tpr, _ = roc_curve(y, y_prob)\n",
    "#         roc_auc = auc(fpr, tpr)\n",
    "#         fig, ax = plt.subplots()\n",
    "#         ax.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "#         ax.plot([0, 1], [0, 1], 'k--')\n",
    "#         ax.set_xlim([0.0, 1.0])\n",
    "#         ax.set_ylim([0.0, 1.05])\n",
    "#         ax.set_xlabel('False Positive Rate')\n",
    "#         ax.set_ylabel('True Positive Rate')\n",
    "#         ax.set_title(f'{method_name} - {name} ROC Curve')\n",
    "#         ax.legend(loc=\"lower right\")\n",
    "#         save_plot(fig, f'{method_name}_{name}_ROC_Curve.png')\n",
    "\n",
    "#     # Save metrics to CSV\n",
    "#     metrics_df = pd.DataFrame(metrics_list)\n",
    "#     metrics_df.to_csv(os.path.join(output_dir, f'{method_name}_metrics.csv'), index=False)\n",
    "\n",
    "#     return results\n",
    "\n",
    "# # Plot Accuracy and Loss\n",
    "# def plot_metrics(name, y, y_pred):\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "#     # Accuracy Plot\n",
    "#     sns.barplot(x=['Accuracy'], y=[accuracy_score(y, y_pred)], ax=axes[0])\n",
    "#     axes[0].set_title(f'{name} Accuracy')\n",
    "\n",
    "#     # Loss Plot (1 - Accuracy)\n",
    "#     sns.barplot(x=['Loss'], y=[1 - accuracy_score(y, y_pred)], ax=axes[1])\n",
    "#     axes[1].set_title(f'{name} Loss')\n",
    "\n",
    "#     save_plot(fig, f'{name}_Accuracy_Loss.png')\n",
    "\n",
    "# # Plot Equal Error Rate (EER)\n",
    "# def plot_eer(y, y_prob, name):\n",
    "#     display = DetCurveDisplay.from_predictions(y, y_prob)\n",
    "#     fpr, fnr = display.fpr, display.fnr\n",
    "#     eer = fpr[np.nanargmin(np.abs(fpr - fnr))]\n",
    "#     fig = plt.figure()\n",
    "#     plt.title(f'{name} DET Curve (EER = {eer:.2f})')\n",
    "#     display.plot()\n",
    "#     save_plot(fig, f'{name}_DET_Curve_EER.png')\n",
    "#     print(f'{name} Equal Error Rate (EER): {eer:.2f}')\n",
    "\n",
    "# # Generate Plots for Each Model\n",
    "# def generate_plots_for_all_models(results, method_name):\n",
    "#     for name, result in results.items():\n",
    "#         plot_metrics(f'{method_name} - {name}', labels, result['y_pred'])\n",
    "#         plot_eer(labels, result['y_prob'], f'{method_name} - {name}')\n",
    "\n",
    "# # Evaluate and Plot for Each Dimensionality Reduction Method\n",
    "# pca_results = evaluate_model(X_pca, labels, 'PCA')\n",
    "# generate_plots_for_all_models(pca_results, 'PCA')\n",
    "\n",
    "# kpca_results = evaluate_model(X_kpca, labels, 'Kernel PCA')\n",
    "# generate_plots_for_all_models(kpca_results, 'Kernel PCA')\n",
    "\n",
    "# lda_results = evaluate_model(X_lda, labels, 'LDA')\n",
    "# generate_plots_for_all_models(lda_results, 'LDA')\n",
    "\n",
    "# tsne_results = evaluate_model(X_tsne, labels, 't-SNE')\n",
    "# generate_plots_for_all_models(tsne_results, 't-SNE')\n",
    "\n",
    "\n",
    "# ipca_results = evaluate_model(X_ipca, labels, 'IPCA')\n",
    "# generate_plots_for_all_models(ipca_results, 'IPCA')\n",
    "\n",
    "# grp_results = evaluate_model(X_grp, labels, 'GRP')\n",
    "# generate_plots_for_all_models(grp_results, 'GRP')\n",
    "\n",
    "# umap_results = evaluate_model(X_umap, labels, 'UMAP')\n",
    "# generate_plots_for_all_models(umap_results, 'UMAP')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81fa060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "# from sklearn.decomposition import KernelPCA, IncrementalPCA\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "# from sklearn.random_projection import GaussianRandomProjection\n",
    "# import umap\n",
    "# from openpyxl import Workbook\n",
    "\n",
    "# # Create a directory to save plots and metrics if it doesn't exist\n",
    "# output_dir = \"model_outputs\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# features = scaler.fit_transform(features)\n",
    "\n",
    "# # Dimensionality Reduction and Feature Selection Techniques\n",
    "# def apply_kernel_pca(X, n_components=300, kernel='rbf'):\n",
    "#     kpca = KernelPCA(n_components=n_components, kernel=kernel)\n",
    "#     return kpca.fit_transform(X)\n",
    "\n",
    "# def apply_ipca(X, n_components=5, batch_size=500):\n",
    "#     ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
    "#     return ipca.fit_transform(X)\n",
    "\n",
    "# # def apply_grp(X, n_components=10):\n",
    "# #     grp = GaussianRandomProjection(n_components=n_components)\n",
    "# #     return grp.fit_transform(X)\n",
    "\n",
    "# def apply_umap(X, n_components=10):\n",
    "#     umap_model = umap.UMAP(n_components=n_components)\n",
    "#     return umap_model.fit_transform(X)\n",
    "\n",
    "# # Dimensionality Reduction\n",
    "# # X_kpca = apply_kernel_pca(features)\n",
    "# X_ipca = apply_ipca(features)\n",
    "# # X_grp = apply_grp(features)\n",
    "# X_umap = apply_umap(features)\n",
    "\n",
    "# # Classification Algorithms\n",
    "# models = {\n",
    "#     'RandomForest': RandomForestClassifier(),\n",
    "#     'KNN': KNeighborsClassifier(),\n",
    "#     'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "# }\n",
    "\n",
    "# # 5-Fold Cross Validation\n",
    "# cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# def save_plot(fig, filename):\n",
    "#     file_path = os.path.join(output_dir, filename)\n",
    "#     fig.savefig(file_path)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# def calculate_eer(fpr, tpr):\n",
    "#     # Calculate the point where FPR and TPR are closest to the diagonal (EER)\n",
    "#     eer = fpr[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n",
    "#     return eer\n",
    "\n",
    "# def evaluate_model(X, y, method_name):\n",
    "#     results = {}\n",
    "#     metrics_list = []\n",
    "\n",
    "#     # Split the data into train and test sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "#     for name, model in models.items():\n",
    "#         # Train the model and predict on both train and test sets\n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_train_pred = model.predict(X_train)\n",
    "#         y_test_pred = model.predict(X_test)\n",
    "\n",
    "#         # Calculate metrics for train set\n",
    "#         train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "#         train_precision = precision_score(y_train, y_train_pred)\n",
    "#         train_recall = recall_score(y_train, y_train_pred)\n",
    "#         train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "#         # Calculate metrics for test set\n",
    "#         test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "#         test_precision = precision_score(y_test, y_test_pred)\n",
    "#         test_recall = recall_score(y_test, y_test_pred)\n",
    "#         test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "#         # Calculate ROC curve and AUC for test set\n",
    "#         fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "#         roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#         # Calculate EER for test set\n",
    "#         eer = calculate_eer(fpr, tpr)\n",
    "\n",
    "#         # Print and collect metrics\n",
    "#         print(f'{method_name} - {name} - Train Accuracy: {train_accuracy}')\n",
    "#         print(f'{method_name} - {name} - Train Precision: {train_precision}')\n",
    "#         print(f'{method_name} - {name} - Train Recall: {train_recall}')\n",
    "#         print(f'{method_name} - {name} - Train F1 Score: {train_f1}')\n",
    "#         print(f'{method_name} - {name} - Test Accuracy: {test_accuracy}')\n",
    "#         print(f'{method_name} - {name} - Test Precision: {test_precision}')\n",
    "#         print(f'{method_name} - {name} - Test Recall: {test_recall}')\n",
    "#         print(f'{method_name} - {name} - Test F1 Score: {test_f1}')\n",
    "#         print(f'{method_name} - {name} - Test AUC: {roc_auc}')\n",
    "#         print(f'{method_name} - {name} - Test EER: {eer}')\n",
    "\n",
    "#         metrics_list.append({\n",
    "#             'Method': method_name,\n",
    "#             'Model': name,\n",
    "#             'Dimensionality Reduction Method': method_name,\n",
    "#             'Train Accuracy': train_accuracy,\n",
    "#             'Train Precision': train_precision,\n",
    "#             'Train Recall': train_recall,\n",
    "#             'Train F1 Score': train_f1,\n",
    "#             'Test Accuracy': test_accuracy,\n",
    "#             'Test Precision': test_precision,\n",
    "#             'Test Recall': test_recall,\n",
    "#             'Test F1 Score': test_f1,\n",
    "#             'Test AUC': roc_auc,\n",
    "#             'Test EER': eer\n",
    "#         })\n",
    "\n",
    "#         # Storing Results\n",
    "#         results[name] = {\n",
    "#             'model': model,\n",
    "#             'y_train_pred': y_train_pred,\n",
    "#             'y_test_pred': y_test_pred\n",
    "#         }\n",
    "\n",
    "#     # Save metrics to Excel\n",
    "#     metrics_df = pd.DataFrame(metrics_list)\n",
    "#     metrics_df.to_excel(os.path.join(output_dir, f'{method_name}_metrics.xlsx'), index=False)\n",
    "\n",
    "#     return results\n",
    "\n",
    "# # Evaluate for each Dimensionality Reduction Method\n",
    "# # kpca_results = evaluate_model(X_kpca, labels, 'Kernel PCA')\n",
    "# ipca_results = evaluate_model(X_ipca, labels, 'IPCA')\n",
    "# # grp_results = evaluate_model(X_grp, labels, 'GRP')\n",
    "# umap_results = evaluate_model(X_umap, labels, 'UMAP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47ffb9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPCA - RandomForest - Train Accuracy: 1.0\n",
      "IPCA - RandomForest - Train Precision: 1.0\n",
      "IPCA - RandomForest - Train Recall: 1.0\n",
      "IPCA - RandomForest - Train F1 Score: 1.0\n",
      "IPCA - RandomForest - Test Accuracy: 0.9615384615384616\n",
      "IPCA - RandomForest - Test Precision: 0.9925373134328358\n",
      "IPCA - RandomForest - Test Recall: 0.9300699300699301\n",
      "IPCA - RandomForest - Test F1 Score: 0.9602888086642599\n",
      "IPCA - RandomForest - Test AUC: 0.9865274585554306\n",
      "IPCA - RandomForest - Test EER: 0.04895104895104895\n",
      "IPCA - KNN - Train Accuracy: 0.9310344827586207\n",
      "IPCA - KNN - Train Precision: 0.9140401146131805\n",
      "IPCA - KNN - Train Recall: 0.9522388059701492\n",
      "IPCA - KNN - Train F1 Score: 0.9327485380116959\n",
      "IPCA - KNN - Test Accuracy: 0.9475524475524476\n",
      "IPCA - KNN - Test Precision: 0.9444444444444444\n",
      "IPCA - KNN - Test Recall: 0.951048951048951\n",
      "IPCA - KNN - Test F1 Score: 0.9477351916376306\n",
      "IPCA - KNN - Test AUC: 0.9621741894469167\n",
      "IPCA - KNN - Test EER: 0.055944055944055944\n",
      "IPCA - XGBoost - Train Accuracy: 1.0\n",
      "IPCA - XGBoost - Train Precision: 1.0\n",
      "IPCA - XGBoost - Train Recall: 1.0\n",
      "IPCA - XGBoost - Train F1 Score: 1.0\n",
      "IPCA - XGBoost - Test Accuracy: 0.9615384615384616\n",
      "IPCA - XGBoost - Test Precision: 0.9782608695652174\n",
      "IPCA - XGBoost - Test Recall: 0.9440559440559441\n",
      "IPCA - XGBoost - Test F1 Score: 0.9608540925266904\n",
      "IPCA - XGBoost - Test AUC: 0.980830358452736\n",
      "IPCA - XGBoost - Test EER: 0.055944055944055944\n",
      "UMAP - RandomForest - Train Accuracy: 1.0\n",
      "UMAP - RandomForest - Train Precision: 1.0\n",
      "UMAP - RandomForest - Train Recall: 1.0\n",
      "UMAP - RandomForest - Train F1 Score: 1.0\n",
      "UMAP - RandomForest - Test Accuracy: 0.9230769230769231\n",
      "UMAP - RandomForest - Test Precision: 0.9172413793103448\n",
      "UMAP - RandomForest - Test Recall: 0.9300699300699301\n",
      "UMAP - RandomForest - Test F1 Score: 0.9236111111111112\n",
      "UMAP - RandomForest - Test AUC: 0.9754022201574649\n",
      "UMAP - RandomForest - Test EER: 0.07692307692307693\n",
      "UMAP - KNN - Train Accuracy: 0.9160419790104948\n",
      "UMAP - KNN - Train Precision: 0.9240121580547113\n",
      "UMAP - KNN - Train Recall: 0.9074626865671642\n",
      "UMAP - KNN - Train F1 Score: 0.9156626506024096\n",
      "UMAP - KNN - Test Accuracy: 0.9125874125874126\n",
      "UMAP - KNN - Test Precision: 0.8986486486486487\n",
      "UMAP - KNN - Test Recall: 0.9300699300699301\n",
      "UMAP - KNN - Test F1 Score: 0.9140893470790378\n",
      "UMAP - KNN - Test AUC: 0.9586043327302068\n",
      "UMAP - KNN - Test EER: 0.1048951048951049\n",
      "UMAP - XGBoost - Train Accuracy: 1.0\n",
      "UMAP - XGBoost - Train Precision: 1.0\n",
      "UMAP - XGBoost - Train Recall: 1.0\n",
      "UMAP - XGBoost - Train F1 Score: 1.0\n",
      "UMAP - XGBoost - Test Accuracy: 0.9195804195804196\n",
      "UMAP - XGBoost - Test Precision: 0.8947368421052632\n",
      "UMAP - XGBoost - Test Recall: 0.951048951048951\n",
      "UMAP - XGBoost - Test F1 Score: 0.9220338983050848\n",
      "UMAP - XGBoost - Test AUC: 0.9688004303388918\n",
      "UMAP - XGBoost - Test EER: 0.08391608391608392\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "import umap\n",
    "\n",
    "# Create a directory to save plots and metrics if it doesn't exist\n",
    "output_dir = \"model_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# Dimensionality Reduction and Feature Selection Techniques\n",
    "def apply_ipca(X, n_components=5, batch_size=500):\n",
    "    ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
    "    return ipca.fit_transform(X)\n",
    "\n",
    "def apply_umap(X, n_components=10):\n",
    "    umap_model = umap.UMAP(n_components=n_components)\n",
    "    return umap_model.fit_transform(X)\n",
    "\n",
    "# Dimensionality Reduction\n",
    "X_ipca = apply_ipca(features)\n",
    "X_umap = apply_umap(features)\n",
    "\n",
    "# Classification Algorithms\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# 5-Fold Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def save_plot(fig, filename):\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    fig.savefig(file_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "def calculate_eer(fpr, tpr):\n",
    "    # Calculate the point where FPR and TPR are closest to the diagonal (EER)\n",
    "    eer = fpr[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n",
    "    return eer\n",
    "\n",
    "def plot_metrics(train_metrics, test_metrics, metric_name, method_name, model_name):\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax.plot(metrics, train_metrics, marker='o', linestyle='-', color='blue', label='Train')\n",
    "    ax.plot(metrics, test_metrics, marker='o', linestyle='-', color='green', label='Test')\n",
    "\n",
    "    ax.set_title(f'{method_name} - {model_name} - {metric_name}')\n",
    "    ax.set_xlabel('Metrics')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    save_plot(fig, f'{method_name}_{model_name}_{metric_name}.png')\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, roc_auc, method_name, model_name):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    ax.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_title(f'{method_name} - {model_name} - ROC Curve')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    save_plot(fig, f'{method_name}_{model_name}_ROC.png')\n",
    "\n",
    "def evaluate_model(X, y, method_name):\n",
    "    results = {}\n",
    "    metrics_list = []\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        # Train the model and predict on both train and test sets\n",
    "        model.fit(X_train, y_train)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics for train set\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        train_precision = precision_score(y_train, y_train_pred)\n",
    "        train_recall = recall_score(y_train, y_train_pred)\n",
    "        train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "        # Calculate metrics for test set\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        test_precision = precision_score(y_test, y_test_pred)\n",
    "        test_recall = recall_score(y_test, y_test_pred)\n",
    "        test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "        # Calculate ROC curve and AUC for test set\n",
    "        fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Calculate EER for test set\n",
    "        eer = calculate_eer(fpr, tpr)\n",
    "\n",
    "        # Plotting Metrics\n",
    "        plot_metrics([train_accuracy, train_precision, train_recall, train_f1],\n",
    "                     [test_accuracy, test_precision, test_recall, test_f1],\n",
    "                     \"Metrics\", method_name, name)\n",
    "\n",
    "        # Plot ROC Curve\n",
    "        plot_roc_curve(fpr, tpr, roc_auc, method_name, name)\n",
    "\n",
    "        # Print and collect metrics\n",
    "        print(f'{method_name} - {name} - Train Accuracy: {train_accuracy}')\n",
    "        print(f'{method_name} - {name} - Train Precision: {train_precision}')\n",
    "        print(f'{method_name} - {name} - Train Recall: {train_recall}')\n",
    "        print(f'{method_name} - {name} - Train F1 Score: {train_f1}')\n",
    "        print(f'{method_name} - {name} - Test Accuracy: {test_accuracy}')\n",
    "        print(f'{method_name} - {name} - Test Precision: {test_precision}')\n",
    "        print(f'{method_name} - {name} - Test Recall: {test_recall}')\n",
    "        print(f'{method_name} - {name} - Test F1 Score: {test_f1}')\n",
    "        print(f'{method_name} - {name} - Test AUC: {roc_auc}')\n",
    "        print(f'{method_name} - {name} - Test EER: {eer}')\n",
    "\n",
    "        metrics_list.append({\n",
    "            'Method': method_name,\n",
    "            'Model': name,\n",
    "            'Dimensionality Reduction Method': method_name,\n",
    "            'Train Accuracy': train_accuracy,\n",
    "            'Train Precision': train_precision,\n",
    "            'Train Recall': train_recall,\n",
    "            'Train F1 Score': train_f1,\n",
    "            'Test Accuracy': test_accuracy,\n",
    "            'Test Precision': test_precision,\n",
    "            'Test Recall': test_recall,\n",
    "            'Test F1 Score': test_f1,\n",
    "            'Test AUC': roc_auc,\n",
    "            'Test EER': eer\n",
    "        })\n",
    "\n",
    "        # Storing Results\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'y_train_pred': y_train_pred,\n",
    "            'y_test_pred': y_test_pred\n",
    "        }\n",
    "\n",
    "    # Save metrics to Excel\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    metrics_df.to_excel(os.path.join(output_dir, f'{method_name}_metrics.xlsx'), index=False)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Evaluate for each Dimensionality Reduction Method\n",
    "ipca_results = evaluate_model(X_ipca, labels, 'IPCA')\n",
    "umap_results = evaluate_model(X_umap, labels, 'UMAP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1461c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
